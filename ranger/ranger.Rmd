## ``ranger`` Ranger

#### Description
Ranger is a fast implementation of random forests (Breiman 2001) or recursive partitioning, particularly
suited for high dimensional data. Classification, regression, and survival forests are supported.
Classification and regression forests are implemented as in the original Random Forest
(Breiman 2001), survival forests as in Random Survival Forests (Ishwaran et al. 2008). Includes
implementations of extremely randomized trees (Geurts et al. 2006) and quantile regression forests
(Meinshausen 2006).

#### Usage
<pre><code>
ranger(formula = NULL, data = NULL, num.trees = 500, mtry = NULL,
importance = "none", write.forest = TRUE, probability = FALSE,
min.node.size = NULL, replace = TRUE, sample.fraction = ifelse(replace,
1, 0.632), case.weights = NULL, class.weights = NULL, splitrule = NULL,
num.random.splits = 1, alpha = 0.5, minprop = 0.1,
split.select.weights = NULL, always.split.variables = NULL,
respect.unordered.factors = NULL, scale.permutation.importance = FALSE,
keep.inbag = FALSE, holdout = FALSE, quantreg = FALSE,
num.threads = NULL, save.memory = FALSE, verbose = TRUE, seed = NULL,
dependent.variable.name = NULL, status.variable.name = NULL,
classification = NULL)
</code></pre>

#### Arguments
* formula Object of class formula or character describing the model to fit. Interaction
terms supported only for numerical variables.
data Training data of class data.frame, matrix, dgCMatrix (Matrix) or gwaa.data
(GenABEL).
* num.trees Number of trees.
* mtry Number of variables to possibly split at in each node. Default is the (rounded
down) square root of the number variables.
* importance Variable importance mode, one of ’none’, ’impurity’, ’impurity_corrected’, ’permutation’.
The ’impurity’ measure is the Gini index for classification, the variance
of the responses for regression and the sum of test statistics (see splitrule)
for survival.
* write.forest Save ranger.forest object, required for prediction. Set to FALSE to reduce
memory #### Usage if no prediction intended.
probability Grow a probability forest as in Malley et al. (2012).
min.node.size Minimal node size. Default 1 for classification, 5 for regression, 3 for survival,
and 10 for probability.
* replace Sample with replacement.
* sample.fraction Fraction of observations to sample. Default is 1 for sampling with replacement
and 0.632 for sampling without replacement. For classification, this can be a
vector of class-specific values.
* case.weights Weights for sampling of training observations. Observations with larger weights
will be selected with higher probability in the bootstrap (or subsampled) samples
for the trees.
* class.weights Weights for the outcome classes (in order of the factor levels) in the splitting
rule (cost sensitive learning). Classification and probability prediction only. For
classification the weights are also applied in the majority vote in terminal nodes.
splitrule Splitting rule. For classification and probability estimation "gini" or "extratrees"
with default "gini". For regression "variance", "extratrees" or "maxstat" with
default "variance". For survival "logrank", "extratrees", "C" or "maxstat" with
default "logrank".
* num.random.splits
For "extratrees" splitrule.: Number of random splits to consider for each candidate
splitting variable.
* alpha For "maxstat" splitrule: Significance threshold to allow splitting.
* minprop For "maxstat" splitrule: Lower quantile of covariate distribution to be considered
for splitting.
* split.select.weights
Numeric vector with weights between 0 and 1, representing the probability to
select variables for splitting. Alternatively, a list of size num.trees, containing
split select weight vectors for each tree can be used.
16 ranger
* always.split.variables
Character vector with variable names to be always selected in addition to the
mtry variables tried for splitting.
respect.unordered.factors
Handling of unordered factor covariates. One of ’ignore’, ’order’ and ’partition’.
For the "extratrees" splitrule the default is "partition" for all other splitrules ’ignore’.
Alternatively TRUE (=’order’) or FALSE (=’ignore’) can be used. See
below for #### Details.
scale.permutation.importance
Scale permutation importance by standard error as in (Breiman 2001). Only
applicable if permutation variable importance mode selected.
keep.inbag Save how often observations are in-bag in each tree.
holdout Hold-out mode. Hold-out all samples with case weight 0 and use these for
variable importance and prediction error.
quantreg Prepare quantile prediction as in quantile regression forests (Meinshausen 2006).
Regression only. Set keep.inbag = TRUE to prepare out-of-bag quantile prediction.
num.threads Number of threads. Default is number of CPUs available.
save.memory Use memory saving (but slower) splitting mode. No effect for survival and
GWAS data. Warning: This option slows down the tree growing, use only if you
encounter memory problems.
verbose Show computation status and estimated runtime.
seed Random seed. Default is NULL, which generates the seed from R. Set to 0 to
ignore the R seed.
dependent.variable.name
Name of dependent variable, needed if no formula given. For survival forests
this is the time variable.
status.variable.name
Name of status variable, only applicable to survival data and needed if no formula
given. Use 1 for event and 0 for censoring.
classification Only needed if data is a matrix. Set to TRUE to grow a classification forest.
#### Details
The tree type is determined by the type of the dependent variable. For factors classification trees are
grown, for numeric values regression trees and for survival objects survival trees. The Gini index is
used as default splitting rule for classification. For regression, the estimated response variances or
maximally selected rank statistics (Wright et al. 2016) can be used. For Survival the log-rank test,
a C-index based splitting rule (Schmid et al. 2015) and maximally selected rank statistics (Wright
et al. 2016) are available. For all tree types, forests of extremely randomized trees (Geurts et al.
2006) can be grown.
With the probability option and factor dependent variable a probability forest is grown. Here,
the node impurity is used for splitting, as in classification forests. Predictions are class probabilities
for each sample. In contrast to other implementations, each tree returns a probability estimate and
these estimates are averaged for the forest probability estimate. For #### Details see Malley et al. (2012).
ranger 17
Note that for classification and regression nodes with size smaller than min.node.size can occur,
as in original Random Forests. For survival all nodes contain at min.node.size samples.
Variables selected with always.split.variables are tried additionally to the mtry variables randomly
selected. In split.select.weights variables weighted with 0 are never selected and variables
with 1 are always selected. Weights do not need to sum up to 1, they will be normalized
later. The weights are assigned to the variables in the order they appear in the formula or in the
data if no formula is used. Names of the split.select.weights vector are ignored. The #### Usage of
split.select.weights can increase the computation times for large forests.
Unordered factor covariates can be handled in 3 different ways by using respect.unordered.factors:
For ’ignore’ all factors are regarded ordered, for ’partition’ all possible 2-partitions are considered
for splitting. For ’order’ and 2-class classification the factor levels are ordered by their proportion
falling in the second class, for regression by their mean response, as described in Hastie et al.
(2009), chapter 9.2.4. For multiclass classification the factor levels are ordered by the first principal
component of the weighted covariance matrix of the contingency table (Coppersmith et al. 1999),
for survival by the median survival (or the largest available quantile if the median is not available).
The use of ’order’ is recommended, as it computationally fast and can handle an unlimited number
of factor levels. Note that the factors are only reordered once and not again in each split.
The ’impurity_corrected’ importance measure is unbiased in terms of the number of categories and
category frequencies and is almost as fast as the standard impurity importance. It is a modified
version of the method by Sandri & Zuccolotto (2008), which is faster and more memory efficient.
See Nembrini et al. (2018) for #### Details. This importance measure can be combined with the methods
to estimate p-values in importance_pvalues.
For a large number of variables and data frames as input data the formula interface can be slow
or impossible to use. Alternatively dependent.variable.name (and status.variable.name for
survival) can be used. Consider setting save.memory = TRUE if you encounter memory problems
for very large datasets, but be aware that this option slows down the tree growing.
For GWAS data consider combining ranger with the GenABEL package. See the Examples section
below for a demonstration using Plink data. All SNPs in the GenABEL object will be used for
splitting. To use only the SNPs without sex or other covariates from the phenotype file, use 0 on
the right hand side of the formula. Note that missing values are treated as an extra category while
splitting.
See https://github.com/imbs-hl/ranger for the development version.
With recent R versions, multithreading on Windows platforms should just work. If you compile
yourself, the new RTools toolchain is required.
Value
Object of class ranger with elements
forest Saved forest (If write.forest set to TRUE). Note that the variable IDs in the
split.varIDs object do not necessarily represent the column number in R.
predictions Predicted classes/values, based on out of bag samples (classification and regression
only).
* variable.importance
Variable importance for each independent variable.
* 
prediction.error
Overall out of bag prediction error. For classification this is the fraction of missclassified
samples, for probability estimation the Brier score, for regression the
mean squared error and for survival one minus Harrell’s C-index.
r.squared R squared. Also called explained variance or coefficient of determination (regression
only). Computed on out of bag data.
* confusion.matrix
Contingency table for classes and predictions based on out of bag samples (classification
only).
* unique.death.times
Unique death times (survival only).
chf Estimated cumulative hazard function for each sample (survival only).
survival Estimated survival function for each sample (survival only).
call Function call.
* num.trees Number of trees.
* num.independent.variables
Number of independent variables.
* mtry Value of mtry used.
* min.node.size Value of minimal node size used.
* treetype Type of forest/tree. classification, regression or survival.
* importance.mode
Importance mode used.
* num.samples Number of samples.
* inbag.counts Number of times the observations are in-bag in the trees.

#### Author(s)
Marvin N. Wright


#### Examples
```{r}
require(ranger)
## Classification forest with default settings
ranger(Species ~ ., data = iris)
## Prediction
train.idx <- sample(nrow(iris), 2/3 * nrow(iris))
iris.train <- iris[train.idx, ]
iris.test <- iris[-train.idx, ]
rg.iris <- ranger(Species ~ ., data = iris.train)
pred.iris <- predict(rg.iris, data = iris.test)
table(iris.test$Species, pred.iris$predictions)
## Quantile regression forest
rf <- ranger(mpg ~ ., mtcars[1:26, ], quantreg = TRUE)
pred <- predict(rf, mtcars[27:32, ], type = "quantiles")
pred$predictions
## Variable importance
rg.iris <- ranger(Species ~ ., data = iris, importance = "impurity")
rg.iris$variable.importance
## Survival forest
require(survival)
rg.veteran <- ranger(Surv(time, status) ~ ., data = veteran)
plot(rg.veteran$unique.death.times, rg.veteran$survival[1,])
20 timepoints.ranger
## Alternative interface
ranger(dependent.variable.name = "Species", data = iris)
## Not run:
## Use GenABEL interface to read Plink data into R and grow a classification forest
## The ped and map files are not included
library(GenABEL)
convert.snp.ped("data.ped", "data.map", "data.raw")
dat.gwaa <- load.gwaa.data("data.pheno", "data.raw")
phdata(dat.gwaa)$trait <- factor(phdata(dat.gwaa)$trait)
ranger(trait ~ ., data = dat.gwaa)
## End(Not run)
```

#### References
• Wright, M. N. & Ziegler, A. (2017). ranger: A fast implementation of random forests for high
dimensional data in C++ and R. J Stat Softw 77:1-17. http://dx.doi.org/10.18637/jss.
v077.i01.
• Schmid, M., Wright, M. N. & Ziegler, A. (2016). On the use of Harrell’s C for clinical risk
prediction via random survival forests. Expert Syst Appl 63:450-459. http://dx.doi.org/
10.1016/j.eswa.2016.07.018.
• Wright, M. N., Dankowski, T. & Ziegler, A. (2017). Unbiased split variable selection for
random survival forests using maximally selected rank statistics. Stat Med 36:1272-1284.
http://dx.doi.org/10.1002/sim.7212.
• Nembrini, S., Koenig, I. R. & Wright, M. N. (2018). The revival of the Gini Importance?
Bioinformatics. http://dx.doi.org/10.1093/bioinformatics/bty373.
• Breiman, L. (2001). Random forests. Mach Learn, 45:5-32. http://dx.doi.org/10.1023/
A:1010933404324.
ranger 19
• Ishwaran, H., Kogalur, U. B., Blackstone, E. H., & Lauer, M. S. (2008). Random survival
forests. Ann Appl Stat 2:841-860. http://dx.doi.org/10.1097/JTO.0b013e318233d835.
• Malley, J. D., Kruppa, J., Dasgupta, A., Malley, K. G., & Ziegler, A. (2012). Probability
machines: consistent probability estimation using nonparametric learning machines. Methods
Inf Med 51:74-81. http://dx.doi.org/10.3414/ME00-01-0052.
• Hastie, T., Tibshirani, R., Friedman, J. (2009). The Elements of Statistical Learning. Springer,
New York. 2nd edition.
• Geurts, P., Ernst, D., Wehenkel, L. (2006). Extremely randomized trees. Mach Learn 63:3-42.
http://dx.doi.org/10.1007/s10994-006-6226-1.
• Meinshausen (2006). Quantile Regression Forests. J Mach Learn Res 7:983-999. http:
//www.jmlr.org/papers/v7/meinshausen06a.html.
• Sandri, M. & Zuccolotto, P. (2008). A bias correction algorithm for the Gini variable importance
measure in classification trees. J Comput Graph Stat, 17:611-628. http://dx.doi.
org/10.1198/106186008X344522.
• Coppersmith D., Hong S. J., Hosking J. R. (1999). Partitioning nominal attributes in decision
trees. Data Min Knowl Discov 3:197-217. http://dx.doi.org/10.1023/A:1009869804967.
See Also
predict.ranger
