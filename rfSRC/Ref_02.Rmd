plot.survival Plot of Survival Estimates
Description
Plot various survival estimates.
Usage
## S3 method for class 'rfsrc'
plot.survival(x, plots.one.page = TRUE,
show.plots = TRUE, subset, collapse = FALSE,
haz.model = c("spline", "ggamma", "nonpar", "none"),
k = 25, span = "cv", cens.model = c("km", "rfsrc"), ...)
Arguments
x An object of class (rfsrc, grow) or (rfsrc, predict).
plots.one.page Should plots be placed on one page?
show.plots Should plots be displayed?
subset Vector indicating which individuals we want estimates for. All individuals are
used if not specified.
collapse Collapse the survival and cumulative hazard function across the individuals
specified by ‘subset’? Only applies when ‘subset’ is specified.
haz.model Method for estimating the hazard. See details below. Applies only when ‘subset’
is specified.
k The number of natural cubic spline knots used for estimating the hazard function.
Applies only when ‘subset’ is specified.
span The fraction of the observations in the span of Friedman’s super-smoother used
for estimating the hazard function. Applies only when ‘subset’ is specified.
26 plot.survival
cens.model Method for estimating the censoring distribution used in the inverse probability
of censoring weights (IPCW) for the Brier score:
km: Uses the Kaplan-Meier estimator.
rfscr: Uses random survival forests.
... Further arguments passed to or from other methods.
Details
If ‘subset’ is not specified, generates the following three plots (going from top to bottom, left to
right):
1. Forest estimated survival function for each individual (thick red line is overall ensemble survival,
thick green line is Nelson-Aalen estimator).
2. Brier score (0=perfect, 1=poor, and 0.25=guessing) stratified by ensemble mortality. Based
on the IPCW method described in Gerds et al. (2006). Stratification is into 4 groups corresponding
to the 0-25, 25-50, 50-75 and 75-100 percentile values of mortality. Red line is the
overall (non-stratified) Brier score.
3. Plot of mortality of each individual versus observed time. Points in blue correspond to events,
black points are censored observations.
When ‘subset’ is specified, then for each individual in ‘subset’, the following three plots are
generated:
1. Forest estimated survival function.
2. Forest estimated cumulative hazard function (CHF) (displayed using black lines). Blue lines
are the CHF from the estimated hazard function. See the next item.
3. A smoothed hazard function derived from the forest estimated CHF (or survival function).
The default method, ‘haz.model="spline"’, models the log CHF using natural cubic splines
as described in Royston and Parmar (2002). The lasso is used for model selection, implemented
using the glmnet package (this package must be installed for this option to work).
If ‘haz.model="ggamma"’, a three-parameter generalized gamma distribution (using the parameterization
described in Cox et al, 2007) is fit to the smoothed forest survival function,
where smoothing is imposed using Friedman’s supersmoother (implemented by supsmu). If
‘haz.model="nonpar"’, Friedman’s supersmoother is applied to the forest estimated hazard
function (obtained by taking the crude derivative of the smoothed forest CHF). Finally, setting
‘haz.model="none"’ suppresses hazard estimation and no hazard estimate is provided.
At this time, please note that all hazard estimates are considered experimental and users should
interpret the results with caution.
Note that when the object x is of class (rfsrc, predict) not all plots will be produced. In
particular, Brier scores are not calculated.
Only applies to survival families. In particular, fails for competing risk analyses. Use plot.competing.risk
in such cases.
Whenever possible, out-of-bag (OOB) values are used.
plot.survival 27
Value
Invisibly, the conditional and unconditional Brier scores, and the integrated Brier score (if they are
available).
Author(s)
Hemant Ishwaran and Udaya B. Kogalur
References
Cox C., Chu, H., Schneider, M. F. and Munoz, A. (2007). Parametric survival analysis and taxonomy
of hazard functions for the generalized gamma distribution. Statistics in Medicine 26:4252-
4374.
Gerds T.A and Schumacher M. (2006). Consistent estimation of the expected Brier score in general
survival models with right-censored event times, Biometrical J., 6:1029-1040.
Graf E., Schmoor C., Sauerbrei W. and Schumacher M. (1999). Assessment and comparison of
prognostic classification schemes for survival data, Statist. in Medicine, 18:2529-2545.
Ishwaran H. and Kogalur U.B. (2007). Random survival forests for R, Rnews, 7(2):25-31.
Royston P. and Parmar M.K.B. (2002). Flexible parametric proportional-hazards and proportionalodds
models for censored survival data, with application to prognostic modelling and estimation of
treatment effects, Statist. in Medicine, 21::2175-2197.
See Also
plot.competing.risk, predict.rfsrc, rfsrc
Examples
## veteran data
data(veteran, package = "randomForestSRC")
plot.survival(rfsrc(Surv(time, status)~ ., veteran), cens.model = "rfsrc")
## pbc data
data(pbc, package = "randomForestSRC")
pbc.obj <- rfsrc(Surv(days, status) ~ ., pbc)
# default spline approach
plot.survival(pbc.obj, subset = 3)
plot.survival(pbc.obj, subset = 3, k = 100)
# three-parameter generalized gamma is approximately the same
# but notice that its CHF estimate (blue line) is not as accurate
plot.survival(pbc.obj, subset = 3, haz.model = "ggamma")
# nonparametric method is too wiggly or undersmooths
plot.survival(pbc.obj, subset = 3, haz.model = "nonpar", span = 0.1)
plot.survival(pbc.obj, subset = 3, haz.model = "nonpar", span = 0.8)
28 plot.variable
plot.variable Plot Marginal Effect of Variables
Description
Plot the marginal effect of an x-variable on the class probability (classification), response (regression),
mortality (survival), or the expected years lost (competing risk). Users can select between
marginal (unadjusted, but fast) and partial plots (adjusted, but slower).
Usage
## S3 method for class 'rfsrc'
plot.variable(x, xvar.names, target,
m.target = NULL, time, surv.type = c("mort", "rel.freq",
"surv", "years.lost", "cif", "chf"), class.type =
c("prob", "bayes"), partial = FALSE, oob = TRUE, show.plots = TRUE,
plots.per.page = 4, granule = 5, sorted = TRUE, nvar, npts = 25,
smooth.lines = FALSE, subset, ...)
Arguments
x An object of class (rfsrc, grow), (rfsrc, synthetic), or (rfsrc, plot.variable).
See the examples below for illustration of the latter.
xvar.names Names of the x-variables to be used.
target For classification, an integer or character value specifying the class to focus on
(defaults to the first class). For competing risks, an integer value between 1 and
J indicating the event of interest, where J is the number of event types. The
default is to use the first event type.
m.target Character value for multivariate families specifying the target outcome to be
used. If left unspecified, the algorithm will choose a default target.
time For survival, the time at which the predicted survival value is evaluated at (depends
on surv.type).
surv.type For survival, specifies the predicted value. See details below.
class.type For classification, specifies the predicted value. See details below.
partial Should partial plots be used?
oob OOB (TRUE) or in-bag (FALSE) predicted values.
show.plots Should plots be displayed?
plots.per.page Integer value controlling page layout.
granule Integer value controlling whether a plot for a specific variable should be treated
as a factor and therefore given as a boxplot. Larger values coerce boxplots.
sorted Should variables be sorted by importance values.
nvar Number of variables to be plotted. Default is all.
plot.variable 29
npts Maximum number of points used when generating partial plots for continuous
variables.
smooth.lines Use lowess to smooth partial plots.
subset Vector indicating which rows of the x-variable matrix x$xvar to use. All rows
are used if not specified. Do not define subset based on the original data (which
could have been processed due to missing values or for other reasons in the
previous forest call) but define subset based on the rows of x$xvar.
... Further arguments passed to or from other methods.
Details
The vertical axis displays the ensemble predicted value, while x-variables are plotted on the horizontal
axis.
1. For regression, the predicted response is used.
2. For classification, it is the predicted class probability specified by ‘target’, or the class of
maximum probability depending on ‘class.type’.
3. For multivariate families, it is the predicted value of the outcome specified by ‘m.target’ and
if that is a classification outcome, by ‘target’.
4. For survival, the choices are:
• Mortality (mort).
• Relative frequency of mortality (rel.freq).
• Predicted survival (surv), where the predicted survival is for the time point specified
using time (the default is the median follow up time).
5. For competing risks, the choices are:
• The expected number of life years lost (years.lost).
• The cumulative incidence function (cif).
• The cumulative hazard function (chf).
In all three cases, the predicted value is for the event type specified by ‘target’. For cif and
chf the quantity is evaluated at the time point specified by time.
For partial plots use ‘partial=TRUE’. Their interpretation are different than marginal plots. The
y-value for a variable X, evaluated at X = x, is
˜f(x) = 1
n
Xn
i=1
ˆf(x, xi,o),
where xi,o represents the value for all other variables other than X for individual i and ˆf is the
predicted value. Generating partial plots can be very slow. Choosing a small value for npts can
speed up computational times as this restricts the number of distinct x values used in computing ˜f.
For continuous variables, red points are used to indicate partial values and dashed red lines indicate
a smoothed error bar of +/- two standard errors. Black dashed line are the partial values.
Set ‘smooth.lines=TRUE’ for lowess smoothed lines. For discrete variables, partial values are indicated
using boxplots with whiskers extending out approximately two standard errors from the
mean. Standard errors are meant only to be a guide and should be interpreted with caution.
30 plot.variable
Partial plots can be slow. Setting ‘npts’ to a smaller number can help.
For greater customization and flexibility in partial plot calls, consider using the function partial.rfsrc
which provides a direct interface for calculating partial plot data.
Author(s)
Hemant Ishwaran and Udaya B. Kogalur
References
Friedman J.H. (2001). Greedy function approximation: a gradient boosting machine, Ann. of
Statist., 5:1189-1232.
Ishwaran H., Kogalur U.B. (2007). Random survival forests for R, Rnews, 7(2):25-31.
Ishwaran H., Kogalur U.B., Blackstone E.H. and Lauer M.S. (2008). Random survival forests, Ann.
App. Statist., 2:841-860.
Ishwaran H., Gerds T.A., Kogalur U.B., Moore R.D., Gange S.J. and Lau B.M. (2014). Random
survival forests for competing risks. Biostatistics, 15(4):757-773.
See Also
rfsrc, rfsrcSyn, partial.rfsrc, predict.rfsrc
Examples
## ------------------------------------------------------------
## survival/competing risk
## ------------------------------------------------------------
## survival
data(veteran, package = "randomForestSRC")
v.obj <- rfsrc(Surv(time,status)~., veteran, ntree = 100)
plot.variable(v.obj, plots.per.page = 3)
plot.variable(v.obj, plots.per.page = 2, xvar.names = c("trt", "karno", "age"))
plot.variable(v.obj, surv.type = "surv", nvar = 1, time = 200)
plot.variable(v.obj, surv.type = "surv", partial = TRUE, smooth.lines = TRUE)
plot.variable(v.obj, surv.type = "rel.freq", partial = TRUE, nvar = 2)
## example of plot.variable calling a pre-processed plot.variable object
p.v <- plot.variable(v.obj, surv.type = "surv", partial = TRUE, smooth.lines = TRUE)
plot.variable(p.v)
p.v$plots.per.page <- 1
p.v$smooth.lines <- FALSE
plot.variable(p.v)
## competing risks
data(follic, package = "randomForestSRC")
follic.obj <- rfsrc(Surv(time, status) ~ ., follic, nsplit = 3, ntree = 100)
plot.variable(follic.obj, target = 2)
predict.rfsrc 31
## ------------------------------------------------------------
## regression
## ------------------------------------------------------------
## airquality
airq.obj <- rfsrc(Ozone ~ ., data = airquality)
plot.variable(airq.obj, partial = TRUE, smooth.lines = TRUE)
plot.variable(airq.obj, partial = TRUE, subset = airq.obj$xvar$Solar.R < 200)
## motor trend cars
mtcars.obj <- rfsrc(mpg ~ ., data = mtcars)
plot.variable(mtcars.obj, partial = TRUE, smooth.lines = TRUE)
## ------------------------------------------------------------
## classification
## ------------------------------------------------------------
## iris
iris.obj <- rfsrc(Species ~., data = iris)
plot.variable(iris.obj, partial = TRUE)
## motor trend cars: predict number of carburetors
mtcars2 <- mtcars
mtcars2$carb <- factor(mtcars2$carb,
labels = paste("carb", sort(unique(mtcars$carb))))
mtcars2.obj <- rfsrc(carb ~ ., data = mtcars2)
plot.variable(mtcars2.obj, partial = TRUE)
## ------------------------------------------------------------
## multivariate regression
## ------------------------------------------------------------
mtcars.mreg <- rfsrc(Multivar(mpg, cyl) ~., data = mtcars)
plot.variable(mtcars.mreg, m.target = "mpg", partial = TRUE, nvar = 1)
plot.variable(mtcars.mreg, m.target = "cyl", partial = TRUE, nvar = 1)
## ------------------------------------------------------------
## multivariate mixed outcomes
## ------------------------------------------------------------
mtcars2 <- mtcars
mtcars2$carb <- factor(mtcars2$carb)
mtcars2$cyl <- factor(mtcars2$cyl)
mtcars.mix <- rfsrc(Multivar(carb, mpg, cyl) ~ ., data = mtcars2)
plot.variable(mtcars.mix, m.target = "cyl", target = "4", partial = TRUE, nvar = 1)
plot.variable(mtcars.mix, m.target = "cyl", target = 2, partial = TRUE, nvar = 1)
predict.rfsrc Prediction for Random Forests for Survival, Regression, and Classification
32 predict.rfsrc
Description
Obtain predicted values using a forest. Also returns performance values if the test data contains
y-outcomes.
Usage
## S3 method for class 'rfsrc'
predict(object,
newdata,
m.target = NULL,
importance = c(FALSE, TRUE, "none", "permute", "random", "anti"),
block.size = NULL,
ensemble = NULL,
na.action = c("na.omit", "na.impute"),
outcome = c("train", "test"),
proximity = FALSE,
forest.wt = FALSE,
ptn.count = 0,
distance = FALSE,
var.used = c(FALSE, "all.trees", "by.tree"),
split.depth = c(FALSE, "all.trees", "by.tree"), seed = NULL,
do.trace = FALSE, membership = FALSE, statistics = FALSE, ...)
Arguments
object An object of class (rfsrc, grow) or (rfsrc, forest).
newdata Test data. If missing, the original grow (training) data is used.
ensemble Optional parameter for specifying the type of ensemble. Can be oob, inbag or
all, although not all choices will be applicable depending on the setting (e.g.
when predicting on newdata there is no notion of out-of-bag).
m.target Character vector for multivariate families specifying the target outcomes to be
used. The default is to use all coordinates.
importance Method for computing variable importance (VIMP) when test data contains youtcomes
values. Also see vimp for more flexibility, including joint vimp calculations.
block.size Should the error rate be calculated on every tree? When NULL, it will only be
calculated on the last tree. To view the error rate on every nth tree, set the
value to an integer between 1 and ntree. If importance is requested, VIMP is
calculated in "blocks" of size equal to block.size, thus resulting in a useful
compromise between ensemble and permutation VIMP.
na.action Missing value action. The default na.omit removes the entire record if even
one of its entries is NA. Selecting ‘na.impute’ imputes the test data.
outcome Determines whether the y-outcomes from the training data or the test data are
used to calculate the predicted value. The default and natural choice is train
which uses the original training data. Option is ignored when newdata is missing
as the training data is used for the test data in such settings. The option is
predict.rfsrc 33
also ignored whenever the test data is devoid of y-outcomes. See the details and
examples below for more information.
proximity Should proximity between test observations be calculated? Possible choices are
"inbag", "oob", "all", TRUE, or FALSE — but some options may not be valid
and will depend on the context of the predict call. The safest choice is TRUE if
proximity is desired.
distance Should distance between test observations be calculated? Possible choices are
"inbag", "oob", "all", TRUE, or FALSE — but some options may not be valid
and will depend on the context of the predict call. The safest choice is TRUE if
distance is desired.
forest.wt Should the forest weight matrix for test observations be calculated? Choices are
the same as proximity.
ptn.count The number of terminal nodes that each tree in the grow forest should be pruned
back to. The terminal node membership for the pruned forest is returned but no
other action is taken. The default is ptn.count=0 which does no pruning.
var.used Record the number of times a variable is split?
split.depth Return minimal depth for each variable for each case?
seed Negative integer specifying seed for the random number generator.
do.trace Number of seconds between updates to the user on approximate time to completion.
membership Should terminal node membership and inbag information be returned?
statistics Should split statistics be returned? Values can be parsed using stat.split.
... Further arguments passed to or from other methods.
Details
Predicted values are obtained by dropping test data down the grow forest (the forest grown using the
training data). The overall error rate and VIMP are also returned if the test data contains y-outcome
values. Single as well as joint VIMP measures can be requested. Note that calculating VIMP can
be computationally expensive (especially when the dimension is high), thus if VIMP is not needed,
computational times can be significantly improved by setting ‘importance="none"’ which turns
VIMP off.
Setting ‘na.action="na.impute"’ imputes missing test data (x-variables and/or y-outcomes). Imputation
uses the grow-forest and only training data is used to impute test data to avoid biasing error
rates and VIMP (Ishwaran et al. 2008). See the rfsrc help file for details.
If no test data is provided, then the original training data is used and the code reverts to restore
mode allowing the user to restore the original grow forest. This is useful, because it gives the user
the ability to extract outputs from the forest that were not asked for in the original grow call.
If ‘outcome="test"’, the predictor is calculated by using y-outcomes from the test data (outcome
information must be present). In this case, the terminal nodes from the grow-forest are recalculated
using the y-outcomes from the test set. This yields a modified predictor in which the topology of
the forest is based solely on the training data, but where the predicted value is based on the test
data. Error rates and VIMP are calculated by bootstrapping the test data and using out-of-bagging
to ensure unbiased estimates. See the examples for illustration.
34 predict.rfsrc
Value
An object of class (rfsrc, predict), which is a list with the following components:
call The original grow call to rfsrc.
family The family used in the analysis.
n Sample size of test data (depends upon NA values).
ntree Number of trees in the grow forest.
yvar Test set y-outcomes or original grow y-outcomes if none.
yvar.names A character vector of the y-outcome names.
xvar Data frame of test set x-variables.
xvar.names A character vector of the x-variable names.
leaf.count Number of terminal nodes for each tree in the grow forest. Vector of length
ntree.
proximity Symmetric proximity matrix of the test data.
forest The grow forest.
membership Matrix recording terminal node membership for the test data where each column
contains the node number that a case falls in for that tree.
inbag Matrix recording inbag membership for the test data where each column contains
the number of times that a case appears in the bootstrap sample for that
tree.
var.used Count of the number of times a variable was used in growing the forest.
imputed.indv Vector of indices of records in test data with missing values.
imputed.data Data frame comprising imputed test data. The first columns are the y-outcomes
followed by the x-variables.
split.depth Matrix [i][j] or array [i][j][k] recording the minimal depth for variable [j] for
case [i], either averaged over the forest, or by tree [k].
node.stats Split statistics returned when statistics=TRUE which can be parsed using
stat.split.
err.rate Cumulative OOB error rate for the test data if y-outcomes are present.
importance Test set variable importance (VIMP). Can be NULL.
predicted Test set predicted value.
predicted.oob OOB predicted value (NULL unless ‘outcome="test"’).
++++++++ for classification settings, additionally ++++++++
class In-bag predicted class labels.
class.oob OOB predicted class labels (NULL unless ‘outcome="test"’).
++++++++ for multivariate settings, additionally ++++++++
predict.rfsrc 35
regrOutput List containing performance values for test multivariate regression responses
(applies only in multivariate settings).
clasOutput List containing performance values for test multivariate categorical (factor) responses
(applies only in multivariate settings).
++++++++ for survival settings, additionally ++++++++
chf Cumulative hazard function (CHF).
chf.oob OOB CHF (NULL unless ‘outcome="test"’).
survival Survival function.
survival.oob OOB survival function (NULL unless ‘outcome="test"’).
time.interest Ordered unique death times.
ndead Number of deaths.
++++++++ for competing risks, additionally ++++++++
chf Cause-specific cumulative hazard function (CSCHF) for each event.
chf.oob OOB CSCHF for each event (NULL unless ‘outcome="test"’).
cif Cumulative incidence function (CIF) for each event.
cif.oob OOB CIF for each event (NULL unless ‘outcome="test"’).
time.interest Ordered unique event times.
ndead Number of events.
Note
The dimensions and values of returned objects depend heavily on the underlying family and whether
y-outcomes are present in the test data. In particular, items related to performance will be NULL
when y-outcomes are not present. For multivariate families, predicted values, VIMP, error rate, and
performance values are stored in the lists regrOutput and clasOutput.
For more detailed information regarding returned values (such as predicted) see the rfsrc help
file.
Author(s)
Hemant Ishwaran and Udaya B. Kogalur
References
Breiman L. (2001). Random forests, Machine Learning, 45:5-32.
Ishwaran H., Kogalur U.B., Blackstone E.H. and Lauer M.S. (2008). Random survival forests, Ann.
App. Statist., 2:841-860.
Ishwaran H. and Kogalur U.B. (2007). Random survival forests for R, Rnews, 7(2):25-31.
See Also
plot.competing.risk, plot.rfsrc, plot.survival, plot.variable, rfsrc, stat.split, vimp
36 predict.rfsrc
Examples
## ------------------------------------------------------------
## typical train/testing scenario
## ------------------------------------------------------------
data(veteran, package = "randomForestSRC")
train <- sample(1:nrow(veteran), round(nrow(veteran) * 0.80))
veteran.grow <- rfsrc(Surv(time, status) ~ ., veteran[train, ], ntree = 100)
veteran.pred <- predict(veteran.grow, veteran[-train , ])
print(veteran.grow)
print(veteran.pred)
## ------------------------------------------------------------
## predicted probability and predicted class labels are returned
## in the predict object for classification analyses
## ------------------------------------------------------------
data(breast, package = "randomForestSRC")
breast.obj <- rfsrc(status ~ ., data = breast[(1:100), ])
breast.pred <- predict(breast.obj, breast[-(1:100), ])
print(head(breast.pred$predicted))
print(breast.pred$class)
## ------------------------------------------------------------
## example illustrating restore mode
## if predict is called without specifying the test data
## the original training data is used and the forest is restored
## ------------------------------------------------------------
# first we make the grow call
airq.obj <- rfsrc(Ozone ~ ., data = airquality)
# now we restore it and compare it to the original call
# they are identical
predict(airq.obj)
print(airq.obj)
# we can retrieve various outputs that were not asked for in
# in the original call
#here we extract the proximity matrix
prox <- predict(airq.obj, proximity = TRUE)$proximity
print(prox[1:10,1:10])
#here we extract the number of times a variable was used to grow
#the grow forest
var.used <- predict(airq.obj, var.used = "by.tree")$var.used
print(head(var.used))
## ------------------------------------------------------------
## unique feature of randomForestSRC
predict.rfsrc 37
## cross-validation can be used when factor labels differ over
## training and test data
## ------------------------------------------------------------
# first we convert all x-variables to factors
data(veteran, package = "randomForestSRC")
veteran.factor <- data.frame(lapply(veteran, factor))
veteran.factor$time <- veteran$time
veteran.factor$status <- veteran$status
# split the data into unbalanced train/test data (5/95)
# the train/test data have the same levels, but different labels
train <- sample(1:nrow(veteran), round(nrow(veteran) * .05))
summary(veteran.factor[train,])
summary(veteran.factor[-train,])
# grow the forest on the training data and predict on the test data
veteran.f.grow <- rfsrc(Surv(time, status) ~ ., veteran.factor[train, ])
veteran.f.pred <- predict(veteran.f.grow, veteran.factor[-train , ])
print(veteran.f.grow)
print(veteran.f.pred)
## ------------------------------------------------------------
## example illustrating the flexibility of outcome = "test"
## illustrates restoration of forest via outcome = "test"
## ------------------------------------------------------------
# first we make the grow call
data(pbc, package = "randomForestSRC")
pbc.grow <- rfsrc(Surv(days, status) ~ ., pbc)
# now use predict with outcome = TEST
pbc.pred <- predict(pbc.grow, pbc, outcome = "test")
# notice that error rates are the same!!
print(pbc.grow)
print(pbc.pred)
# note this is equivalent to restoring the forest
pbc.pred2 <- predict(pbc.grow)
print(pbc.grow)
print(pbc.pred)
print(pbc.pred2)
# similar example, but with na.action = "na.impute"
airq.obj <- rfsrc(Ozone ~ ., data = airquality, na.action = "na.impute")
print(airq.obj)
print(predict(airq.obj))
# ... also equivalent to outcome="test" but na.action = "na.impute" required
print(predict(airq.obj, airquality, outcome = "test", na.action = "na.impute"))
# classification example
iris.obj <- rfsrc(Species ~., data = iris)
38 predict.rfsrc
print(iris.obj)
print(predict.rfsrc(iris.obj, iris, outcome = "test"))
## ------------------------------------------------------------
## another example illustrating outcome = "test"
## unique way to check reproducibility of the forest
## ------------------------------------------------------------
# primary call
set.seed(542899)
data(pbc, package = "randomForestSRC")
train <- sample(1:nrow(pbc), round(nrow(pbc) * 0.50))
pbc.out <- rfsrc(Surv(days, status) ~ ., data=pbc[train, ])
# standard predict call
pbc.train <- predict(pbc.out, pbc[-train, ], outcome = "train")
#non-standard predict call: overlays the test data on the grow forest
pbc.test <- predict(pbc.out, pbc[-train, ], outcome = "test")
# check forest reproducibilility by comparing "test" predicted survival
# curves to "train" predicted survival curves for the first 3 individuals
Time <- pbc.out$time.interest
matplot(Time, t(exp(-pbc.train$chf)[1:3,]), ylab = "Survival", col = 1, type = "l")
matlines(Time, t(exp(-pbc.test$chf)[1:3,]), col = 2)
## ------------------------------------------------------------
## survival analysis using mixed multivariate outcome analysis
## compare the predicted value to RSF
## ------------------------------------------------------------
# fit the pbc data using RSF
data(pbc, package = "randomForestSRC")
rsf.obj <- rfsrc(Surv(days, status) ~ ., pbc)
yvar <- rsf.obj$yvar
# fit a mixed outcome forest using days and status as y-variables
pbc.mod <- pbc
pbc.mod$status <- factor(pbc.mod$status)
mix.obj <- rfsrc(Multivar(days, status) ~., pbc.mod)
# compare oob predicted values
rsf.pred <- rsf.obj$predicted.oob
mix.pred <- mix.obj$regrOutput$days$predicted.oob
plot(rsf.pred, mix.pred)
# compare C-index error rate
rsf.err <- randomForestSRC:::cindex(yvar$days, yvar$status, rsf.pred)
mix.err <- 1 - randomForestSRC:::cindex(yvar$days, yvar$status, mix.pred)
cat("RSF :", rsf.err, "\n")
cat("multivariate forest:", mix.err, "\n")
print.rfsrc 39
print.rfsrc Print Summary Output of a RF-SRC Analysis
Description
Print summary output from a RF-SRC analysis. This is the default print method for the package.
Usage
## S3 method for class 'rfsrc'
print(x, outcome.target = NULL, ...)
Arguments
x An object of class (rfsrc, grow), (rfsrc, synthetic), or (rfsrc, predict).
outcome.target Character value for multivariate families specifying the target outcome to be
used. The default is to use the first coordinate.
... Further arguments passed to or from other methods.
Author(s)
Hemant Ishwaran and Udaya B. Kogalur
References
Ishwaran H. and Kogalur U.B. (2007). Random survival forests for R, Rnews, 7/2:25-31.
See Also
rfsrc, rfsrcSyn, predict.rfsrc
Examples
iris.obj <- rfsrc(Species ~., data = iris, ntree=100)
print(iris.obj)
40 quantileReg
quantileReg Quantile Regression Forests
Description
Determines the conditional quantiles and conditional density for a regression forest. Applies to both
univariate and multivariate forests. Can be used for both training and testing purposes.
Usage
## S3 method for class 'rfsrc'
quantileReg(obj, oob = TRUE, prob = (1:10) / 10, newdata = NULL)
Arguments
obj A previously grown forest.
oob Return OOB (out-of-bag) quantiles? If false, in-bag values are returned.
prob Target quantile probabilities.
newdata Test data (optional) over which conditional quantiles are evaluated over.
Details
Given a regression forest, or a multivariate forest with at least one regression outcome, returns the
conditional quantiles for the target outcomes. Also returns the conditional density, which can be
used to calculate conditional moments, such as the mean and standard deviation.
Value
Quantiles for each of the requested probabilities, conditional density (and conditional cdf) for each
unique y-value, are returned for each data point in the training set, or if a test set is provided, the
values are returned for each data point in the test data. If more than one target outcome is available,
the returned object will be a list of length equal to the number of target outcomes.
Author(s)
Hemant Ishwaran and Udaya B. Kogalur
References
Meinshausen N. (2006) Quantile Regression Forests, Journal of Machine Learning Research, 7:983–
999.
See Also
rfsrc
quantileReg 41
Examples
## ------------------------------------------------------------
## regression example
## ------------------------------------------------------------
## grow call, followed by quantileReg call
o <- rfsrc(mpg ~ ., mtcars)
qo <- quantileReg(o, prob = c(0.25, 0.5, 0.75))
## calculate the conditional mean, compare to OOB predicted value
c.mean <- qo$density %*% qo$yunq
print(data.frame(c.mean = c.mean, pred.oob = o$predicted.oob))
## calculate conditional standard deviation
c.std <- sqrt(qo$density %*% qo$yunq^2 - c.mean ^ 2)
quant <- qo$quantile
colnames(quant) <- paste("q", 100 * qo$prob, sep = "")
print(data.frame(quant, c.std))
## ------------------------------------------------------------
## train/test regression example
## ------------------------------------------------------------
## grow call, followed by quantileReg call
o <- rfsrc(mpg ~ ., mtcars[1:20,])
qo <- quantileReg(o, newdata = mtcars[-(1:20),], prob = c(0.25, 0.5, 0.75))
## calculate test set conditional mean and standard deviation
c.mean <- qo$density %*% qo$yunq
c.std <- sqrt(qo$density %*% qo$yunq^2 - c.mean ^ 2)
quant <- qo$quant
colnames(quant) <- paste("q", 100 * qo$prob, sep = "")
print(data.frame(quant, c.mean, c.std))
## ------------------------------------------------------------
## multivariate mixed outcomes example
## ------------------------------------------------------------
dta <- mtcars
dta$cyl <- factor(dta$cyl)
dta$carb <- factor(dta$carb, ordered = TRUE)
o <- rfsrc(cbind(carb, mpg, cyl, disp) ~., data = dta)
qo <- quantileReg(o)
print(head(qo$mpg$quant))
print(head(qo$disp$quant))
## ------------------------------------------------------------
42 quantileReg
## quantile regression plot for Boston Housing data
## ------------------------------------------------------------
if (library("mlbench", logical.return = TRUE)) {
## apply quantile regression to Boston Housing data
data(BostonHousing)
o <- rfsrc(medv ~ ., BostonHousing)
qo <- quantileReg(o, prob = c(0.25, 0.5, 0.75))
## quantile data for plotting
quant.dat <- qo$quant
y <- o$yvar
## quantile regression plot
plot(range(y), range(quant.dat), xlab = "y",
ylab = ".25-.75 Quantiles", type = "n")
jitter.y <- jitter(y, 10)
points(jitter.y, quant.dat[, 2], pch = 15, col = 4, cex = 0.75)
segments(jitter.y, quant.dat[, 2], jitter.y, quant.dat[, 1], col = "grey")
segments(jitter.y, quant.dat[, 2], jitter.y, quant.dat[, 3], col = "grey")
points(jitter.y, quant.dat[, 1], pch = "-", cex = 1)
points(jitter.y, quant.dat[, 3], pch = "-", cex = 1)
abline(0, 1, lty = 2, col = 2)
}
## ------------------------------------------------------------
## example of quantile regression for ordinal data
## ------------------------------------------------------------
## use the wine data for illustration
data(wine, package = "randomForestSRC")
## regression call -- request forest weights
o <- rfsrc(quality ~ ., wine, ntree = 100, forest.wt = "oob")
## run quantile regression/extract "probabilities" = density values
qo <- quantileReg(o, prob = (1:100)/100)
yunq <- qo$yunq
yvar <- factor(cut(o$yvar, c(-1, yunq), labels = yunq))
qo.dens <- qo$density
colnames(qo.dens) <- yunq
qo.class <- randomForestSRC:::bayes.rule(qo.dens)
qo.confusion <- table(yvar, qo.class)
qo.err <- 1-diag(qo.confusion)/rowSums(qo.confusion)
qo.confusion <- cbind(qo.confusion, qo.err)
print(qo.confusion)
cat("Normalized Brier:", 100 * randomForestSRC:::brier(yvar, qo.dens), "\n")
rfsrc 43
rfsrc Random Forests for Survival, Regression, and Classification (RF-SRC)
Description
Fast OpenMP parallel processing unified treatment of Breiman’s random forests (Breiman 2001) for
a variety of data settings. Applies when the y-response is numeric or categorical, yielding Breiman
regression and classification forests, while random survival forests (Ishwaran et al. 2008, 2012) are
grown for right-censored survival and competing risk data. Multivariate regression and classification
responses as well as mixed regression/classification responses are also handled. Also includes
unsupervised forests and quantile regression forests, quantileReg. Different splitting rules invoked
under deterministic or random splitting are available for all families. Variable predictiveness can
be assessed using variable importance (VIMP) measures for single, as well as grouped variables.
Missing data can be imputed on both training and test data; see impute. The forest object, informally
referred to as an RF-SRC object, contains many useful values which can be directly extracted
by the user and/or parsed using additional functions (see the examples below).
This is the main entry point to the randomForestSRC package. Also see rfsrcFast for a fast
implementation of rfsrc.
For more information about this package and OpenMP, use the command package?randomForestSRC.
Usage
rfsrc(formula, data, ntree = 1000,
mtry = NULL, ytry = NULL,
nodesize = NULL, nodedepth = NULL,
splitrule = NULL, nsplit = 10,
importance = c(FALSE, TRUE, "none", "permute", "random", "anti"),
block.size = if (importance == "none" || as.character(importance) == "FALSE") NULL
else 10,
ensemble = c("all", "oob", "inbag"),
bootstrap = c("by.root", "by.node", "none", "by.user"),
samptype = c("swr", "swor"), sampsize = NULL, samp = NULL, membership = FALSE,
na.action = c("na.omit", "na.impute"), nimpute = 1,
ntime, cause,
proximity = FALSE, distance = FALSE, forest.wt = FALSE,
xvar.wt = NULL, yvar.wt = NULL, split.wt = NULL, case.wt = NULL,
forest = TRUE,
var.used = c(FALSE, "all.trees", "by.tree"),
split.depth = c(FALSE, "all.trees", "by.tree"),
seed = NULL,
do.trace = FALSE,
statistics = FALSE,
...)
44 rfsrc
Arguments
formula A symbolic description of the model to be fit. If missing, unsupervised splitting
is implemented.
data Data frame containing the y-outcome and x-variables.
ntree Number of trees in the forest.
mtry Number of variables randomly selected as candidates for splitting a node. The
default is p/3 for regression, where p equals the number of variables. For all
other families (including unsupervised settings), the default is sqrt(p). Values
are always rounded up.
ytry For unsupervised forests, sets the number of randomly selected pseudo-responses
(see below for more details). The default is ytry=1, which selects one pseudoresponse.
nodesize Forest average number of unique cases (data points) in a terminal node. The
defaults are: survival (3), competing risk (6), regression (5), classification (1),
mixed outcomes (3), unsupervised (3). It is recommended to experiment with
different nodesize values.
nodedepth Maximum depth to which a tree should be grown. The default behaviour is that
this parameter is ignored.
splitrule Splitting rule used to grow trees. See below for details.
nsplit Non-negative integer value. When zero or NULL, deterministic splitting for
an x-variable is in effect. When non-zero, a maximum of nsplit split points
are randomly chosen among the possible split points for the x-variable. This
significantly increases speed.
importance Method for computing variable importance (VIMP). Because VIMP is computationally
expensive, the default action is importance="none" (VIMP can always
be recovered later using the functions vimp or predict). Setting importance=TRUE
implements permutation VIMP. See below for more details.
block.size Should the cumulative error rate be calculated on every tree? When NULL, it will
only be calculated on the last tree and the plot of the cumulative error rate will
result in a flat line. To view the cumulative error rate on every nth tree, set the
value to an integer between 1 and ntree. As an intended side effect, if importance
is requested, VIMP is calculated in "blocks" of size equal to block.size,
thus resulting in a useful compromise between ensemble and permutation VIMP.
The default action is to use 10 trees. See VIMP below for more details.
ensemble Specifies the type of ensemble. By default both out-of-bag (OOB) and inbag
ensembles are returned. Always use OOB values for interfence on the training
data.
bootstrap Bootstrap protocol. The default is by.root which bootstraps the data by sampling
with replacement at the root node before growing the tree (for sampling
without replacement, see the option samptype). If by.node is choosen, the data
is bootstrapped at each node during the grow process. If none is chosen, the
data is not bootstrapped at all. If by.user is choosen, the bootstrap specified by
samp is used. It is not possible to return OOB ensembles or prediction error if
by.node or none are in effect.
rfsrc 45
samptype Type of bootstrap when "by.root" is in effect. Choices are swr (sampling with
replacement, the default action) and swor (sampling without replacement).
sampsize Requested size of bootstrap when "by.root" is in effect (if missing the default
action is the usual bootstrap).
samp Bootstrap specification when "by.user" is in effect. This is a array of dim
n x ntree specifying how many times each record appears inbag in the bootstrap
for each tree.
membership Should terminal node membership and inbag information be returned?
na.action Action taken if the data contains NA’s. Possible values are na.omit or na.impute.
The default na.omit removes the entire record if even one of its entries is NA (for
x-variables this applies only to those specifically listed in ’formula’). Selecting
na.impute imputes the data. See below for more details regarding missing data
imputation.
nimpute Number of iterations of the missing data algorithm. Performance measures such
as out-of-bag (OOB) error rates tend to become optimistic if nimpute is greater
than 1.
ntime Integer value used for survival to constrain ensemble calculations to a grid of
ntime time points. Alternatively if a vector of values of length greater than
one is supplied, it is assumed these are the time points to be used to constrain
the calculations (note that the constrained time points used will be the observed
event times closest to the user supplied time points). If no value is specified, the
default action is to use all observed event times.
cause Integer value between 1 and J indicating the event of interest for competing
risks, where J is the number of event types (this option applies only to competing
risks and is ignored otherwise). While growing a tree, the splitting of a node is
restricted to the event type specified by cause. If not specified, the default is
to use a composite splitting rule which is an average over the entire set of event
types (a democratic approach). Users can also pass a vector of non-negative
weights of length J if they wish to use a customized composite split statistic (for
example, passing a vector of ones reverts to the default composite split statistic).
In all instances when cause is set incorrectly, splitting reverts to the default.
Finally, note that regardless of how cause is specified, the returned forest object
always provides estimates for all event types.
proximity Proximity of cases as measured by the frequency of sharing the same terminal
node. This is an nxn matrix, which can be large. Choices are "inbag",
"oob", "all", TRUE, or FALSE. Setting proximity = TRUE is equivalent to
proximity = "inbag".
distance Distance between cases as measured by the ratio of the sum of the count of
edges from each case to their immediate common ancestor node to the sum of
the count of edges from each case to the root node. If the cases are co-terminal
for a tree, this measure is zero and reduces to 1 - the proximity measure for
these cases in a tree. This is an nxn matrix, which can be large. Choices are
"inbag", "oob", "all", TRUE, or FALSE. Setting distance = TRUE is equivalent
to distance = "inbag".
forest.wt Should the forest weight matrix be calculated? Creates an nxn matrix which
can be used for prediction and constructing customized estimators. Choices are
46 rfsrc
similar to proximity: "inbag", "oob", "all", TRUE, or FALSE. The default is
TRUE which is equivalent to "inbag".
xvar.wt Vector of non-negative weights where entry k, after normalizing, is the probability
of selecting variable k as a candidate for splitting a node. Default is to use
uniform weights. Vector must be of dimension p, where p equals the number
of variables, otherwise the default is invoked. It is generally better to use real
weights rather than integers. With larger sizes of p, the slightly different sampling
algorithms used in the two scenarios can result in dramatically different
execution times.
yvar.wt NOT YET IMPLEMENTED: Vector of non-negative weights where entry k,
after normalizing, is the probability of selecting response k as a candidate for
inclusion in the split statistic in unsupervised settings. Default is to use uniform
weights. Vector must be of the same length as the number of respones in the
data set.
split.wt Vector of non-negative weights where entry k, after normalizing, is the multiplier
by which the split statistic for a variable is adjusted. A large value encourages
the node to split on the variable. Default is to use uniform weights. Vector
must be of dimension p, where p equals the number of variables, otherwise the
default is invoked.
case.wt Vector of non-negative weights where entry k, after normalizing, is the probability
of selecting case k as a candidate for the bootstrap. Default is to use uniform
weights. Vector must be of dimension n, where n equals the number of cases in
the processed data set (missing values may be removed, thus altering the original
sample size). It is generally better to use real weights rather than integers.
With larger sizes of n, the slightly different sampling algorithms used in the two
scenarios can result in dramatically different execution times. See the example
below for the breast data set for an illustration of its use for class imbalanced
data.
forest Should the forest object be returned? Used for prediction on new data and required
by many of the functions used to parse the RF-SRC object. It is recommended
not to change the default setting.
var.used Return variables used for splitting? Default is FALSE. Possible values are all.trees
which returns a vector where each element records the number of times a split
occurred on a variable, and by.tree which is a matrix recording the number of
times a split occurred on a variable in a specific tree.
split.depth Records the minimal depth for each variable. Default is FALSE. Possible values
are all.trees which returns a matrix recording the minimal depth for a variable
(columns) for a specific case (rows) averaged over the forest, and by.tree
which returns a three-dimensional array recording minimal depth for a specific
case (first dimension) for a variable (second dimension) for a specific tree (third
dimension).
seed Negative integer specifying seed for the random number generator.
do.trace Number of seconds between updates to the user on approximate time to completion.
statistics Should split statistics be returned? Values can be parsed using stat.split.
... Further arguments passed to or from other methods.
rfsrc 47
Details
1. Families
Do *not* set this value as the package automagically determines the underlying random forest
family from the type of response and the formula supplied. There are eight possible scenarios:
regr, regr+, class, class+, mix+, unsupv, surv, and surv-CR.
(a) Regression forests (regr) for continuous responses.
(b) Multivariate regression forests (regr+) for multivariate continuous responses.
(c) Classification forests (class) for factor responses.
(d) Multivariate classification forests (class+) for multivariate factor responses.
(e) Multivariate mixed forests (mix+) for mixed continuous and factor responses.
(f) Unsupervised forests (unsupv) when there is no response.
(g) Survival forest (surv) for right-censored survival settings.
(h) Competing risk survival forests (surv-CR) for competing risk scenarios.
See below for how to code the response in the two different survival scenarios and for specifying
a multivariate forest formula.
2. Splitrules
Splitrules are set according to the option splitrule as follows:
• Regression analysis:
(a) The default rule is weighted mean-squared error splitting mse (Breiman et al. 1984,
Chapter 8.4).
(b) Unweighted and heavy weighted mean-squared error splitting rules can be invoked
using splitrules mse.unwt and mse.hvwt. Generally mse works best, but see Ishwaran
(2015) for details.
• Multivariate regression analysis: For multivariate regression responses, a composite normalized
mean-squared error splitting rule is used.
• Classification analysis:
(a) The default rule is Gini index splitting gini (Breiman et al. 1984, Chapter 4.3).
(b) Unweighted and heavy weighted Gini index splitting rules can be invoked using
splitrules gini.unwt and gini.hvwt. Generally gini works best, but see Ishwaran
(2015) for details.
• Multivariate classification analysis: For multivariate classification responses, a composite
normalized Gini index splitting rule is used.
• Mixed outcomes analysis: When both regression and classification responses are detected,
a multivariate normalized composite split rule of mean-squared error and Gini
index splitting is invoked. See Tang and Ishwaran (2017) for details.
• Unsupervised analysis: In settings where there is no outcome, unsupervised splitting is
invoked. In this case, the mixed outcome splitting rule (above) is applied. See Mantero
and Ishwaran (2017) for details.
• Survival analysis:
(a) The default rule is logrank which implements log-rank splitting (Segal, 1988; Leblanc
and Crowley, 1993).
(b) logrankscore implements log-rank score splitting (Hothorn and Lausen, 2003).
• Competing risk analysis:
48 rfsrc
(a) The default rule is logrankCR which implements a modified weighted log-rank splitting
rule modeled after Gray’s test (Gray, 1988).
(b) logrank implements weighted log-rank splitting where each event type is treated as
the event of interest and all other events are treated as censored. The split rule is the
weighted value of each of log-rank statistics, standardized by the variance. For more
details see Ishwaran et al. (2014).
• Custom splitting: All families except unsupervised are available for user defined custom
splitting. Some basic C-programming skills are required. The harness for defining these
rules is in splitCustom.c. In this file we give examples of how to code rules for regression,
classification, survival, and competing risk. Each family can support up to sixteen
custom split rules. Specifying splitrule="custom" or splitrule="custom1" will trigger
the first split rule for the family defined by the training data set. Multivariate families
will need a custom split rule for both regression and classification. In the examples, we
demonstrate how the user is presented with the node specific membership. The task is
then to define a split statistic based on that membership. Take note of the instructions in
splitCustom.c on how to register the custom split rules. It is suggested that the existing
custom split rules be kept in place for reference and that the user proceed to develop
splitrule="custom2" and so on. The package must be recompiled and installed for the
custom split rules to become available.
• Random splitting. For all families, pure random splitting can be invoked by setting
splitrule="random". See below for more details regarding randomized splitting rules.
3. Allowable data types
Data types must be real valued, integer, factor or logical – however all except factors are
coerced and treated as if real valued. For ordered x-variable factors, splits are similar to real
valued variables. If the x-variable factor is unordered, a split will move a subset of the levels
in the parent node to the left daughter, and the complementary subset to the right daughter. All
possible complementary pairs are considered and apply to factors with an unlimited number of
levels. However, there is an optimization check to ensure that the number of splits attempted
is not greater than the number of cases in a node (this internal check will override the nsplit
value in random splitting mode if nsplit is large enough; see below for information about
nsplit).
4. Improving computational speed
See the function rfsrcFast for a fast implementation of rfsrc. In general, the key methods
for increasing speed are as follows:
• Randomized splitting rules
Trees tend to favor splits on continuous variables and factors with large numbers of levels
(Loh and Shih, 1997). To mitigate this bias and improve speed, randomized splitting can
be invoked using the option nsplit. If nsplit is set to a non-zero positive integer, then
a maximum of nsplit split points are chosen randomly for each of the mtry variables
within a node and only these points are used to determine the best split. Pure random
splitting can be invoked by setting splitrule="random". In this case, a variable is randomly
selected and the node is split using a random split point (Cutler and Zhao, 2001;
Lin and Jeon, 2006). Note when pure random splitting is in effect, nsplit is set to one.
• Subsampling
Subsampling can be used to reduce the size of the in-sample data used to grow a tree
and therefore can greatly reduce computational load. Subsampling is implemented using
options sampsize and samptype.
rfsrc 49
• Unique time points
For large survival problems, users should consider setting ntime to a reasonably small
value (such as 50 or 100). This constrains ensemble calculations such as survival functions
to a restricted grid of time points of length no more than ntime and considerably
reduces computational times.
• Large number of variables
Use the default setting of importance="none" which turns off variable importance (VIMP)
calculations and considerably reduces computational times when there are a large number
of variables (see below for more details about variable importance). Variable importance
calculations can always be recovered later using functions vimp or predict. Also consider
using the function max.subtree which calculates minimal depth, a measure of the
depth that a variable splits, and yields fast variable selection (Ishwaran, 2010).
• Factors
For coherence, an immutable map is applied to each factor that ensures that factor levels
in the training data set are consistent with the factor levels in any subsequent test data
set. This map is applied to each factor before and after the native C library is executed.
Because of this, if x-variables are all factors, then computational times may become very
long in high dimensional problems. Consider converting factors to real if this is the case.
5. Prediction Error
Prediction error is calculated using OOB data. Performance is measured in terms of meansquared-error
for regression, and misclassification error for classification. A normalized Brier
score (relative to a coin-toss) is also provided upon printing a classification forest.
For survival, prediction error is measured by 1-C, where C is Harrell’s (Harrell et al., 1982)
concordance index. Prediction error is between 0 and 1, and measures how well the predictor
correctly ranks (classifies) two random individuals in terms of survival. A value of 0.5 is no
better than random guessing. A value of 0 is perfect.
When bootstrapping is by.node or none, a coherent OOB subset is not available to assess prediction
error. Thus, all outputs dependent on this are suppressed. In such cases, prediction error
is only available via classical cross-validation (the user will need to use the predict.rfsrc
function).
6. Variable Importance (VIMP)
To calculate VIMP, use the option importance. Classical permutation VIMP is implemented
when permute or TRUE is selected. In this case, OOB cases for a variable x are randomly
permuted and dropped down a tree. VIMP is calculated by comparing OOB prediction performance
for the permuted predictor to the original predictor.
The exact calculation for VIMP depends upon block.size (an integer value between 1 and
ntree) specifying the number of trees in a block used to determine VIMP. When the value is 1,
VIMP is calculated by tree (blocks of size 1). Specifically, the difference between prediction
error under the perturbed predictor and the original predictor is calculated for each tree and
averaged over the forest. This yields the original Breiman-Cutler VIMP (Breiman 2001).
When block.size is set to ntree, VIMP is calculated by comparing the error rate for the
perturbed OOB forest ensemble (using all trees) to the unperturbed OOB forest ensemble
(using all trees). Thus, unlike Breiman-Cutler VIMP, ensemble VIMP does not measure the
tree average effect of x, but rather its overall forest effect. This is called Ishwaran-Kogalur
VIMP (Ishwaran et al. 2008).
A useful compromise between Breiman-Cutler (BC) and Ishwaran-Kogalur (IK) VIMP can be
obtained by setting block.size to a value between 1 and ntree. Smaller values are closer to
50 rfsrc
BC and larger values closer to IK. Smaller generally gives better accuracy, however computational
times will be higher because VIMP is calculated over more blocks.
The option importance permits different ways to perturb a variable. If random is specified,
then instead of permuting x, OOB case are assigned a daughter node randomly whenever a
split on x is encountered. If anti is specified, x is assigned to the opposite node whenever a
split on x is encountered.
Note that the option none turns VIMP off entirely.
Note that the function vimp provides a friendly user interface for extracting VIMP.
7. Multivariate Forests
Multivariate forests are specified by using the multivariate formula interface. Such a call takes
one of two forms:
rfsrc(Multivar(y1, y2, ..., yd) ~ . , my.data, ...)
rfsrc(cbind(y1, y2, ..., yd) ~ . , my.data, ...)
A multivariate normalized composite splitting rule is used to split nodes. The nature of the
outcomes will inform the code as to the type of multivariate forest to be grown; i.e. whether it
is real-valued, categorical, or a combination of both (mixed). Note that performance measures
(when requested) are returned for all outcomes.
8. Unsupervised Forests
In the case where no y-outcomes are present, unsupervised forests can be invoked by one of
two means:
rfsrc(data = my.data)
rfsrc(Unsupervised() ~ ., data = my.data)
To split a tree node, a random subset of ytry variables are selected from the available features,
and these variables function as "pseudo-responses" to be split on. Thus, in unsupervised mode,
the features take turns acting as both target y-outcomes and x-variables for splitting.
More precisely, as in supervised forests, mtry x-variables are randomly selected from the set
of p features for splitting the node. Then on each mtry loop, ytry variables are selected from
the p-1 remaining features to act as the target pseduo-responses to be split on (there are p-1
possibilities because we exclude the currently selected x-variable for the current mtry loop —
also, only pseudo-responses that pass purity checks are used). The split-statistic for splitting
the node on the pseudo-responses using the x-variable is calculated. The best split over the
mtry pairs is used to split the node.
The default value of ytry is 1 but can be increased by the ytry option. A value larger than 1
initiates multivariate splitting. As illustration, consider the call:
rfsrc(data = my.data, ytry = 5, mtry = 10)
This is equivalent to the call:
rfsrc(Unsupervised(5) ~ ., my.data, mtry = 10)
In the above, a node will be split by selecting mtry=10 x-variables, and for each of these a
random subset of 5 features will be selected as the multivariate pseudo-responses. The splitstatistic
is a multivariate normalized composite splitting rule which is applied to each of the
10 multivariate regression problems. The node is split on the variable leading to the best split.
Note that all performance values (error rates, VIMP, prediction) are turned off in unsupervised
mode.
9. Survival, Competing Risks
rfsrc 51
(a) Survival settings require a time and censoring variable which should be identifed in the
formula as the response using the standard Surv formula specification. A typical formula
call looks like:
Surv(my.time, my.status) ~ .
where my.time and my.status are the variables names for the event time and status
variable in the users data set.
(b) For survival forests (Ishwaran et al. 2008), the censoring variable must be coded as a
non-negative integer with 0 reserved for censoring and (usually) 1=death (event). The
event time must be non-negative.
(c) For competing risk forests (Ishwaran et al., 2013), the implementation is similar to survival,
but with the following caveats:
• Censoring must be coded as a non-negative integer, where 0 indicates right-censoring,
and non-zero values indicate different event types. While 0,1,2,..,J is standard, and
recommended, events can be coded non-sequentially, although 0 must always be used
for censoring.
• Setting the splitting rule to logrankscore will result in a survival analysis in which
all events are treated as if they are the same type (indeed, they will coerced as such).
• Generally, competing risks requires a larger nodesize than survival settings.
10. Missing data imputation
Setting na.action="na.impute" imputes missing data (both x and y-variables) using a modification
of the missing data algorithm of Ishwaran et al. (2008). See also Tang and Ishwaran
(2017). Split statistics are calculated using non-misssing data only. If a node splits on a variable
with missing data, the variable’s missing data is imputed by randomly drawing values
from non-missing in-bag data. The purpose of this is to make it possible to assign cases to
daughter nodes based on the split. Following a node split, imputed data are reset to missing
and the process is repeated until terminal nodes are reached. Missing data in terminal nodes
are imputed using in-bag non-missing terminal node data. For integer valued variables and
censoring indicators, imputation uses a maximal class rule, whereas continuous variables and
survival time use a mean rule.
The missing data algorithm can be iterated by setting nimpute to a positive integer greater
than 1. Using only a few iterations are needed to improve accuracy. When the algorithm is
iterated, at the completion of each iteration, missing data is imputed using OOB non-missing
terminal node data which is then used as input to grow a new forest. Note that when the
algorithm is iterated, a side effect is that missing values in the returned objects xvar, yvar are
replaced by imputed values. Further, imputed objects such as imputed.data are set to NULL.
Also, keep in mind that if the algorithm is iterated, performance measures such as error rates
and VIMP become optimistically biased.
Finally, records in which all outcome and x-variable information are missing are removed
from the forest analysis. Variables having all missing values are also removed.
See the function impute for a fast impute interface.
Value
An object of class (rfsrc, grow) with the following components:
call The original call to rfsrc.
family The family used in the analysis.
52 rfsrc
n Sample size of the data (depends upon NA’s, see na.action).
ntree Number of trees grown.
mtry Number of variables randomly selected for splitting at each node.
nodesize Minimum size of terminal nodes.
nodedepth Maximum depth allowed for a tree.
splitrule Splitting rule used.
nsplit Number of randomly selected split points.
yvar y-outcome values.
yvar.names A character vector of the y-outcome names.
xvar Data frame of x-variables.
xvar.names A character vector of the x-variable names.
xvar.wt Vector of non-negative weights specifying the probability used to select a variable
for splitting a node.
split.wt Vector of non-negative weights where entry k, after normalizing, is the multiplier
by which the split statistic for a covariate is adjusted.
cause.wt Vector of weights used for the composite competing risk splitting rule.
leaf.count Number of terminal nodes for each tree in the forest. Vector of length ntree. A
value of zero indicates a rejected tree (can occur when imputing missing data).
Values of one indicate tree stumps.
proximity Proximity matrix recording the frequency of pairs of data points occur within
the same terminal node.
forest If forest=TRUE, the forest object is returned. This object is used for prediction
with new test data sets and is required for other R-wrappers.
forest.wt Forest weight matrix.
membership Matrix recording terminal node membership where each column contains the
node number that a case falls in for that tree.
splitrule Splitting rule used.
inbag Matrix recording inbag membership where each column contains the number of
times that a case appears in the bootstrap sample for that tree.
var.used Count of the number of times a variable is used in growing the forest.
imputed.indv Vector of indices for cases with missing values.
imputed.data Data frame of the imputed data. The first column(s) are reserved for the yresponses,
after which the x-variables are listed.
split.depth Matrix [i][j] or array [i][j][k] recording the minimal depth for variable [j] for
case [i], either averaged over the forest, or by tree [k].
node.stats Split statistics returned when statistics=TRUE which can be parsed using
stat.split.
err.rate Tree cumulative OOB error rate.
importance Variable importance (VIMP) for each x-variable.
predicted In-bag predicted value.
rfsrc 53
predicted.oob OOB predicted value.
++++++++ for classification settings, additionally ++++++++
class In-bag predicted class labels.
class.oob OOB predicted class labels.
++++++++ for multivariate settings, additionally ++++++++
regrOutput List containing performance values for multivariate regression responses (applies
only in multivariate settings).
clasOutput List containing performance values for multivariate categorical (factor) responses
(applies only in multivariate settings).
++++++++ for survival settings, additionally ++++++++
survival In-bag survival function.
survival.oob OOB survival function.
chf In-bag cumulative hazard function (CHF).
chf.oob OOB CHF.
time.interest Ordered unique death times.
ndead Number of deaths.
++++++++ for competing risks, additionally ++++++++
chf In-bag cause-specific cumulative hazard function (CSCHF) for each event.
chf.oob OOB CSCHF.
cif In-bag cumulative incidence function (CIF) for each event.
cif.oob OOB CIF.
time.interest Ordered unique event times.
ndead Number of events.
Note
Values returned depend heavily on the family. In particular, predicted and predicted.oob are the
following values calculated using in-bag and OOB data:
1. For regression, a vector of predicted y-responses.
2. For classification, a matrix with columns containing the estimated class probability for each
class. Performance values and VIMP for classification are reported as a matrix with J+1
columns where J is the number of classes. The first column "all" is the unconditional value for
performance or VIMP, while the remaining columns are performance and VIMP conditioned
on cases corresponding to that class label.
54 rfsrc
3. For survival, a vector of mortality values (Ishwaran et al., 2008) representing estimated risk for
each individual calibrated to the scale of the number of events (as a specific example, if i has a
mortality value of 100, then if all individuals had the same x-values as i, we would expect an
average of 100 events). Also returned are matrices containing the CHF and survival function.
Each row corresponds to an individual’s ensemble CHF or survival function evaluated at each
time point in time.interest.
4. For competing risks, a matrix with one column for each event recording the expected number
of life years lost due to the event specific cause up to the maximum follow up (Ishwaran
et al., 2013). Also returned are the cause-specific cumulative hazard function (CSCHF)
and the cumulative incidence function (CIF) for each event type. These are encoded as a
three-dimensional array, with the third dimension used for the event type, each time point in
time.interest making up the second dimension (columns), and the case (individual) being
the first dimension (rows).
5. For multivariate families, predicted values (and other performance values such as VIMP and
error rates) are stored in the lists regrOutput and clasOutput which can be parsed using the
functions get.mv.error, get.mv.predicted and get.mv.vimp.
Author(s)
Hemant Ishwaran and Udaya B. Kogalur
References
Breiman L., Friedman J.H., Olshen R.A. and Stone C.J. Classification and Regression Trees, Belmont,
California, 1984.
Breiman L. (2001). Random forests, Machine Learning, 45:5-32.
Cutler A. and Zhao G. (2001). Pert-Perfect random tree ensembles. Comp. Sci. Statist., 33: 490-
497.
Gray R.J. (1988). A class of k-sample tests for comparing the cumulative incidence of a competing
risk, Ann. Statist., 16: 1141-1154.
Harrell et al. F.E. (1982). Evaluating the yield of medical tests, J. Amer. Med. Assoc., 247:2543-
2546.
Hothorn T. and Lausen B. (2003). On the exact distribution of maximally selected rank statistics,
Comp. Statist. Data Anal., 43:121-137.
Ishwaran H. (2007). Variable importance in binary regression trees and forests, Electronic J. Statist.,
1:519-537.
Ishwaran H. and Kogalur U.B. (2007). Random survival forests for R, Rnews, 7(2):25-31.
Ishwaran H., Kogalur U.B., Blackstone E.H. and Lauer M.S. (2008). Random survival forests, Ann.
App. Statist., 2:841-860.
Ishwaran H., Kogalur U.B., Gorodeski E.Z, Minn A.J. and Lauer M.S. (2010). High-dimensional
variable selection for survival data. J. Amer. Statist. Assoc., 105:205-217.
Ishwaran H., Kogalur U.B., Chen X. and Minn A.J. (2011). Random survival forests for highdimensional
data. Stat. Anal. Data Mining, 4:115-132
Ishwaran H., Gerds T.A., Kogalur U.B., Moore R.D., Gange S.J. and Lau B.M. (2014). Random
survival forests for competing risks. Biostatistics, 15(4):757-773.
rfsrc 55
Ishwaran H. and Malley J.D. (2014). Synthetic learning machines. BioData Mining, 7:28.
Ishwaran H. (2015). The effect of splitting on random forests. Machine Learning, 99:75-118.
Ishwaran H. and Lu M. (2018). Standard errors and confidence intervals for variable importance in
random forest regression, classification, and survival. Statistics in Medicine (in press).
Lin Y. and Jeon Y. (2006). Random forests and adaptive nearest neighbors, J. Amer. Statist. Assoc.,
101:578-590.
LeBlanc M. and Crowley J. (1993). Survival trees by goodness of split, J. Amer. Statist. Assoc.,
88:457-467.
Loh W.-Y and Shih Y.-S (1997). Split selection methods for classification trees, Statist. Sinica,
7:815-840.
Mantero A. and Ishwaran H. (2017). Unsupervised random forests.
Mogensen, U.B, Ishwaran H. and Gerds T.A. (2012). Evaluating random forests for survival analysis
using prediction error curves, J. Statist. Software, 50(11): 1-23.
O’Brien R. and Ishwaran H. (2017). A random forests quantile classifier for class imbalanced data.
Segal M.R. (1988). Regression trees for censored data, Biometrics, 44:35-47.
Tang F. and Ishwaran H. (2017). Random forest missing data algorithms. Statistical Analysis and
Data Mining, 10, 363-377.
See Also
find.interaction,
impute, max.subtree,
plot.competing.risk, plot.rfsrc, plot.survival, plot.variable, predict.rfsrc, print.rfsrc,
quantileReg, rfsrcFast, rfsrcSyn,
subsample,
stat.split, tune, var.select, vimp
Examples
##------------------------------------------------------------
## Survival analysis
##------------------------------------------------------------
## veteran data
## randomized trial of two treatment regimens for lung cancer
data(veteran, package = "randomForestSRC")
v.obj <- rfsrc(Surv(time, status) ~ ., data = veteran,
ntree = 100, block.size = 1)
## print and plot the grow object
print(v.obj)
plot(v.obj)
## plot survival curves for first 10 individuals -- direct way
matplot(v.obj$time.interest, 100 * t(v.obj$survival.oob[1:10, ]),
xlab = "Time", ylab = "Survival", type = "l", lty = 1)
56 rfsrc
## plot survival curves for first 10 individuals -- use wrapper
plot.survival(v.obj, subset = 1:10)
## Primary biliary cirrhosis (PBC) of the liver
data(pbc, package = "randomForestSRC")
pbc.obj <- rfsrc(Surv(days, status) ~ ., pbc)
print(pbc.obj)
##------------------------------------------------------------
## Example of imputation in survival analysis
##------------------------------------------------------------
data(pbc, package = "randomForestSRC")
pbc.obj2 <- rfsrc(Surv(days, status) ~ ., pbc,
nsplit = 10, na.action = "na.impute")
## same as above but we iterate the missing data algorithm
pbc.obj3 <- rfsrc(Surv(days, status) ~ ., pbc,
na.action = "na.impute", nimpute = 3)
## fast way to impute the data (no inference is done)
## see impute for more details
pbc.imp <- impute(Surv(days, status) ~ ., pbc, splitrule = "random")
##------------------------------------------------------------
## Compare RF-SRC to Cox regression
## Illustrates C-index and Brier score measures of performance
## assumes "pec" and "survival" libraries are loaded
##------------------------------------------------------------
if (library("survival", logical.return = TRUE)
& library("pec", logical.return = TRUE)
& library("prodlim", logical.return = TRUE))
{
##prediction function required for pec
predictSurvProb.rfsrc <- function(object, newdata, times, ...){
ptemp <- predict(object,newdata=newdata,...)$survival
pos <- sindex(jump.times = object$time.interest, eval.times = times)
p <- cbind(1,ptemp)[, pos + 1]
if (NROW(p) != NROW(newdata) || NCOL(p) != length(times))
stop("Prediction failed")
p
}
## data, formula specifications
data(pbc, package = "randomForestSRC")
pbc.na <- na.omit(pbc) ##remove NA's
surv.f <- as.formula(Surv(days, status) ~ .)
rfsrc 57
pec.f <- as.formula(Hist(days,status) ~ 1)
## run cox/rfsrc models
## for illustration we use a small number of trees
cox.obj <- coxph(surv.f, data = pbc.na)
rfsrc.obj <- rfsrc(surv.f, pbc.na, ntree = 150)
## compute bootstrap cross-validation estimate of expected Brier score
## see Mogensen, Ishwaran and Gerds (2012) Journal of Statistical Software
set.seed(17743)
prederror.pbc <- pec(list(cox.obj,rfsrc.obj), data = pbc.na, formula = pec.f,
splitMethod = "bootcv", B = 50)
print(prederror.pbc)
plot(prederror.pbc)
## compute out-of-bag C-index for cox regression and compare to rfsrc
rfsrc.obj <- rfsrc(surv.f, pbc.na)
cat("out-of-bag Cox Analysis ...", "\n")
cox.err <- sapply(1:100, function(b) {
if (b%%10 == 0) cat("cox bootstrap:", b, "\n")
train <- sample(1:nrow(pbc.na), nrow(pbc.na), replace = TRUE)
cox.obj <- tryCatch({coxph(surv.f, pbc.na[train, ])}, error=function(ex){NULL})
if (is.list(cox.obj)) {
randomForestSRC:::cindex(pbc.na$days[-train],
pbc.na$status[-train],
predict(cox.obj, pbc.na[-train, ]))
} else NA
})
cat("\n\tOOB error rates\n\n")
cat("\tRSF : ", rfsrc.obj$err.rate[rfsrc.obj$ntree], "\n")
cat("\tCox regression : ", mean(cox.err, na.rm = TRUE), "\n")
}
##------------------------------------------------------------
## Competing risks
##------------------------------------------------------------
## WIHS analysis
## cumulative incidence function (CIF) for HAART and AIDS stratified by IDU
data(wihs, package = "randomForestSRC")
wihs.obj <- rfsrc(Surv(time, status) ~ ., wihs, nsplit = 3, ntree = 100)
plot.competing.risk(wihs.obj)
cif <- wihs.obj$cif.oob
Time <- wihs.obj$time.interest
idu <- wihs$idu
cif.haart <- cbind(apply(cif[,,1][idu == 0,], 2, mean),
apply(cif[,,1][idu == 1,], 2, mean))
cif.aids <- cbind(apply(cif[,,2][idu == 0,], 2, mean),
apply(cif[,,2][idu == 1,], 2, mean))
matplot(Time, cbind(cif.haart, cif.aids), type = "l",
lty = c(1,2,1,2), col = c(4, 4, 2, 2), lwd = 3,
ylab = "Cumulative Incidence")
58 rfsrc
legend("topleft",
legend = c("HAART (Non-IDU)", "HAART (IDU)", "AIDS (Non-IDU)", "AIDS (IDU)"),
lty = c(1,2,1,2), col = c(4, 4, 2, 2), lwd = 3, cex = 1.5)
## illustrates the various splitting rules
## illustrates event specific and non-event specific variable selection
if (library("survival", logical.return = TRUE)) {
## use the pbc data from the survival package
## events are transplant (1) and death (2)
data(pbc, package = "survival")
pbc$id <- NULL
## modified Gray's weighted log-rank splitting
pbc.cr <- rfsrc(Surv(time, status) ~ ., pbc)
## log-rank event-one specific splitting
pbc.log1 <- rfsrc(Surv(time, status) ~ ., pbc,
splitrule = "logrank", cause = c(1,0), importance = TRUE)
## log-rank event-two specific splitting
pbc.log2 <- rfsrc(Surv(time, status) ~ ., pbc,
splitrule = "logrank", cause = c(0,1), importance = TRUE)
## extract VIMP from the log-rank forests: event-specific
## extract minimal depth from the Gray log-rank forest: non-event specific
var.perf <- data.frame(md = max.subtree(pbc.cr)$order[, 1],
vimp1 = 100 * pbc.log1$importance[ ,1],
vimp2 = 100 * pbc.log2$importance[ ,2])
print(var.perf[order(var.perf$md), ])
}
## ------------------------------------------------------------
## Regression analysis
## ------------------------------------------------------------
## New York air quality measurements
airq.obj <- rfsrc(Ozone ~ ., data = airquality, na.action = "na.impute")
# partial plot of variables (see plot.variable for more details)
plot.variable(airq.obj, partial = TRUE, smooth.lines = TRUE)
## motor trend cars
mtcars.obj <- rfsrc(mpg ~ ., data = mtcars)
## ------------------------------------------------------------
## Classification analysis
## ------------------------------------------------------------
rfsrc 59
## Edgar Anderson's iris data
iris.obj <- rfsrc(Species ~., data = iris)
## Wisconsin prognostic breast cancer data
data(breast, package = "randomForestSRC")
breast.obj <- rfsrc(status ~ ., data = breast, block.size=1)
plot(breast.obj)
## ------------------------------------------------------------
## Classification analysis with class imbalanced data
## ------------------------------------------------------------
data(breast, package = "randomForestSRC")
breast <- na.omit(breast)
o <- rfsrc(status ~ ., data = breast)
print(o)
## The data is imbalanced so we use balanced random forests
## with undersampling of the majority class
##
## Specifically let n0, n1 be sample sizes for majority, minority
## cases. We sample 2 x n1 cases with majority, minority cases chosen
## with probabilities n1/n, n0/n where n=n0+n1
y <- breast$status
o <- rfsrc(status ~ ., data = breast,
case.wt = randomForestSRC:::make.wt(y),
sampsize = randomForestSRC:::make.size(y))
print(o)
## ------------------------------------------------------------
## Unsupervised analysis
## ------------------------------------------------------------
# two equivalent ways to implement unsupervised forests
mtcars.unspv <- rfsrc(Unsupervised() ~., data = mtcars)
mtcars2.unspv <- rfsrc(data = mtcars)
## ------------------------------------------------------------
## Multivariate regression analysis
## ------------------------------------------------------------
mtcars.mreg <- rfsrc(Multivar(mpg, cyl) ~., data = mtcars,
block.size=1, importance = TRUE)
## extract error rates, vimp, and OOB predicted values for all targets
err <- get.mv.error(mtcars.mreg)
vmp <- get.mv.vimp(mtcars.mreg)
pred <- get.mv.predicted(mtcars.mreg)
## standardized error and vimp
60 rfsrc.news
err.std <- get.mv.error(mtcars.mreg, standardize = TRUE)
vmp.std <- get.mv.vimp(mtcars.mreg, standardize = TRUE)
## ------------------------------------------------------------
## Mixed outcomes analysis
## ------------------------------------------------------------
mtcars.new <- mtcars
mtcars.new$cyl <- factor(mtcars.new$cyl)
mtcars.new$carb <- factor(mtcars.new$carb, ordered = TRUE)
mtcars.mix <- rfsrc(cbind(carb, mpg, cyl) ~., data = mtcars.new, block.size=1)
print(mtcars.mix, outcome.target = "mpg")
print(mtcars.mix, outcome.target = "cyl")
plot(mtcars.mix, outcome.target = "mpg")
plot(mtcars.mix, outcome.target = "cyl")
## ------------------------------------------------------------
## Custom splitting using the pre-coded examples
## ------------------------------------------------------------
## motor trend cars
mtcars.obj <- rfsrc(mpg ~ ., data = mtcars, splitrule = "custom")
## iris analysis
iris.obj <- rfsrc(Species ~., data = iris, splitrule = "custom1")
## WIHS analysis
wihs.obj <- rfsrc(Surv(time, status) ~ ., wihs, nsplit = 3,
ntree = 100, splitrule = "custom1")
