get.split(stat.obj, "disp")
subsample Subsample Forests for VIMP Confidence Intervals
Description
Use subsampling to calculate confidence intervals and standard errors for VIMP (variable importance).
Applies to all families.
subsample 69
Usage
## S3 method for class 'rfsrc'
subsample(obj,
B = 100,
block.size = 1,
subratio = NULL,
stratify = TRUE,
joint = FALSE,
bootstrap = FALSE,
verbose = TRUE)
Arguments
obj A forest grow object.
B Number of subsamples (or number of bootstraps).
block.size Specifies number of trees in a block when calculating VIMP. This is over-ridden
if VIMP is present in the original grow call in which case the grow value is used.
subratio Ratio of subsample size to original sample size. The default is the inverse square
root of the sample size.
stratify Use stratified subsampling? See details below.
joint Include the VIMP for all variables jointly perturbed? This is useful reference
problems where one might be suspicious that many (or all) variables are noise.
bootstrap Use double bootstrap approach in place of subsampling? Much slower, but potentially
more accurate.
verbose Provide verbose output?
Details
Given a forest object, subsamples the forest to obtain standard errors and confidence intervals for
VIMP (Ishwaran and Lu, 2018). If bootstrapping is requested, then the double bootstrap is applied
in place of subsampling.
If VIMP is not present in the original forest object, the algorithm will first need to calculate VIMP.
Therefore, if the user plans to make repeated calls to subsample, it is advisable to include VIMP in
the original grow call. Note that the subsampled forest inherits the same tuning parameters as the
original forests. While a sophisticated algorithm is utilized to acquire as many of the original forest
parameters as possible to be applied to the subsampled forest, there are some conditions where this
will fail: for example there are certain settings where the user has specified non-standard sampling
in the grow forest.
Delete-d jackknife estimators (Shao and Wu, 1989) are returned along with subsampling estimators
(Politis and Romano, 1994). While these two methods are closely related, standard errors for deleted
estimators are generally larger than the subsampled estimates, which is a form of bias correction,
which occurs primarily for variables with true signal. Confidence interval coverage is generally
better under delete-d estimators. Note that undercoverage for strong variables and overcoverage
for noise variables exhibited by both estimators may be beneficial if the goal is variable selection
(Ishwaran and Lu, 2018).
70 subsample
By default, stratified subsampling is used for classification, survival, and competing risk families.
For classification, stratification is on the class label, while for survival and competing risk, stratification
is on the event type and censoring. Users are discouraged from over-riding this option,
especially in small sample settings, as this could lead to error due to subsampled data not having
full representation of class labels in classification settings, and in survival settings, subsampled
data may be devoid of deaths and/or have reduced number of competing risks. Finally, note that
stratified sampling is not available for multivariate families in which case users should especially
exercise caution when selecting subsampling rates.
Note that subsampling and bootstrapping do not take into account missing data imputation that may
have been performed on the forest grow object. In such cases there is no guarantee that standard
errors and confidence intervals will be accurate.
The function extract.subsample is useful for studying the subsampled object. This function has
been exported for the convenience of users to experiment with.
When printing and or plotting results, the default setting is to standardize VIMP, where for regression
families, VIMP is standardized by dividing by the variance and multiplying by 100. For all
other families, VIMP is scaled by 100. This can be turned off using the option standardize in
those wrappers.
Value
A list with the following key components:
rf Original forest grow object.
vmp Variable importance subsampled values.
Author(s)
Hemant Ishwaran and Udaya B. Kogalur
References
Ishwaran H. and Lu M. (2018). Standard errors and confidence intervals for variable importance in
random forest regression, classification, and survival. Statistics in Medicine (in press).
Politis, D.N. and Romano, J.P. (1994). Large sample confidence regions based on subsamples under
minimal assumptions. The Annals of Statistics, 22(4):2031-2050.
Shao, J. and Wu, C.J. (1989). A general theory for jackknife variance estimation. The Annals of
Statistics, 17(3):1176-1197.
See Also
plot.subsample, rfsrc, vimp
Examples
## ------------------------------------------------------------
## regression example
## ------------------------------------------------------------
subsample 71
## grow the forest - request VIMP
reg.o <- rfsrc(mpg ~ ., mtcars)
## very small sample size so need largish subratio
reg.smp.o <- subsample(reg.o, B = 100, subratio = .5)
## plot confidence regions
plot.subsample(reg.smp.o)
## summary of results
print(reg.smp.o)
## now try the double bootstrap (slow!!)
reg.dbs.o <- subsample(reg.o, B = 100, bootstrap = TRUE)
print(reg.dbs.o)
plot.subsample(reg.dbs.o)
## ------------------------------------------------------------
## classification example
## ------------------------------------------------------------
## 3 non-linear, 15 linear, and 5 noise variables
if (library("caret", logical.return = TRUE)) {
d <- twoClassSim(1000, linearVars = 15, noiseVars = 5)
## VIMP based on (default) misclassification error
cls.o <- rfsrc(Class ~ ., d)
cls.smp.o <- subsample(cls.o, B = 100)
plot.subsample(cls.smp.o, cex = .7)
## same as above, but with VIMP defined using normalized Brier score
cls.o2 <- rfsrc(Class ~ ., d, perf.type = "brier")
cls.smp.o2 <- subsample(cls.o2, B = 100)
plot.subsample(cls.smp.o2, cex = .7)
}
## ------------------------------------------------------------
## survival example
## ------------------------------------------------------------
data(pbc, package = "randomForestSRC")
srv.o <- rfsrc(Surv(days, status) ~ ., pbc)
srv.smp.o <- subsample(srv.o, B = 100)
plot.subsample(srv.smp.o)
## ------------------------------------------------------------
## competing risk example
## target event is death (event = 2)
## ------------------------------------------------------------
if (library("survival", logical.return = TRUE)) {
data(pbc, package = "survival")
72 tune
pbc$id <- NULL
cr.o <- rfsrc(Surv(time, status) ~ ., pbc, splitrule = "logrank", cause = 2)
cr.smp.o <- subsample(cr.o, B = 100)
plot.subsample(cr.smp.o, target = 2)
}
## ------------------------------------------------------------
## multivariate family
## ------------------------------------------------------------
if (library("mlbench", logical.return = TRUE)) {
## simulate the data
data(BostonHousing)
bh <- BostonHousing
bh$rm <- factor(round(bh$rm))
o <- rfsrc(cbind(medv, rm) ~ ., bh)
so <- subsample(o)
plot(so)
plot(so, m.target = "rm")
}
## ------------------------------------------------------------
## largish data example - use rfsrcFast for fast forests
## ------------------------------------------------------------
if (library("caret", logical.return = TRUE)) {
## largish data set
d <- twoClassSim(1000, linearVars = 15, noiseVars = 5)
## use a subsampled forest with Brier score performance
o <- rfsrcFast(Class ~ ., d, ntree = 100, perf.type = "brier")
so <- subsample(o, B = 100)
plot.subsample(so, cex = .7)
}
tune Tune Random Forest for the optimal mtry and nodesize parameters
Description
Finds the optimal mtry and nodesize tuning parameter for a random forest using out-of-bag (OOB)
error. Applies to all families.
Usage
## S3 method for class 'rfsrc'
tune(formula, data,
tune 73
mtryStart = ncol(data) / 2,
nodesizeTry = c(1:9, seq(10, 100, by = 5)), ntreeTry = 50,
stepFactor = 1.25, improve = 1e-3, strikeout = 3, maxIter = 25,
trace = FALSE, doBest = TRUE, ...)
Arguments
formula A symbolic description of the model to be fit.
data Data frame containing the y-outcome and x-variables.
mtryStart Starting value of mtry.
nodesizeTry Values of nodesize optimized over.
ntreeTry Number of trees used for the tuning step.
stepFactor At each iteration, mtry is inflated (or deflated) by this value.
improve The (relative) improvement in OOB error must be by this much for the search to
continue.
strikeout The search is discontinued when the relative improvement in OOB error is negative.
However strikeout allows for some tolerance in this. If a negative improvement
is noted a total of strikeout times, the search is stopped. Increase
this value only if you want an exhaustive search.
maxIter The maximum number of iterations allowed for each mtry bisection search.
trace Print the progress of the search?
doBest Return a forest fit with the optimal mtry and nodesize parameters?
... Further options to be passed to rfsrcFast.
Details
Returns a matrix whose first and second columns contain the nodesize and mtry values searched
and whose third column is the corresponding OOB error. Uses standardized OOB error and in the
case of multivariate forests it is the averaged standardized OOB error over the outcomes and for
competing risks it is the averaged standardized OOB error over the event types.
If doBest=TRUE, also returns a forest object fit using the optimal mtry and nodesize values.
All calculations (including the final optimized forest) are based on the fast forest interface rfsrcFast.
Using rfsrcFast allows the optimization strategy to be implemented quickly, however the solution
can only be considered approximate. Users may wish to tweak various options to improve stability.
For example, increasing ntreeTry (which is set to 50 for speed) may help. It is also useful to
look at contour plots of the OOB error as a function of mtry and nodesize (see example below) to
identify regions of the parameter space where error rate is small.
Author(s)
Hemant Ishwaran and Udaya B. Kogalur
See Also
rfsrcFast
74 var.select
Examples
## ------------------------------------------------------------
## White wine classification example
## ------------------------------------------------------------
## load the data
data(wine, package = "randomForestSRC")
wine$quality <- factor(wine$quality)
## default tuning call
o <- tune(quality ~ ., wine)
## here is the optimized forest
print(o$rf)
## visualize the nodesize/mtry OOB surface
if (library("akima", logical.return = TRUE)) {
## nice little wrapper for plotting results
plot.tune <- function(o, linear = TRUE) {
x <- o$results[,1]
y <- o$results[,2]
z <- o$results[,3]
so <- interp(x=x, y=y, z=z, linear = linear)
idx <- which.min(z)
x0 <- x[idx]
y0 <- y[idx]
filled.contour(x = so$x,
y = so$y,
z = so$z,
xlim = range(so$x, finite = TRUE) + c(-2, 2),
ylim = range(so$y, finite = TRUE) + c(-2, 2),
color.palette =
colorRampPalette(c("yellow", "red")),
xlab = "nodesize",
ylab = "mtry",
main = "OOB error for nodesize and mtry",
key.title = title(main = "OOB error", cex.main = 1),
plot.axes = {axis(1);axis(2);points(x0,y0,pch="x",cex=1,font=2);
points(x,y,pch=16,cex=.25)})
}
## plot the surface
plot.tune(o)
}
